<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="description" content="Comparing Agent Interfaces for Web-based Agents on the WebMall Benchmark" />
    <meta name="keywords" content="WebMall, Agent Interfaces, RAG, MCP, NLWeb, LLM Agents, Benchmark Evaluation" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>MCP vs RAG vs NLWeb vs HTML: An Experimental Comparison of Different Agent Interfaces to the Web</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/favicon.ico" />

    <style>
      .results-table {
        position: relative;
        width: 90vw;
        max-width: 1400px;
        left: 50%;
        transform: translateX(-50%);
        overflow-x: auto;
        margin: 1.5rem 0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        border-radius: 8px;
      }

      .results-table table {
        width: 100%;
        table-layout: auto;
        font-size: 0.9rem;
        border-collapse: separate;
        border-spacing: 0;
        background: white;
        border-radius: 8px;
        overflow: hidden;
      }

      .results-table th,
      .results-table td {
        padding: 0.75em 0.5em;
        text-align: center;
        border-bottom: 1px solid #e5e7eb;
        vertical-align: middle;
        white-space: nowrap;
      }

      .results-table th:first-child,
      .results-table td:first-child {
        text-align: left;
        white-space: normal;
        word-wrap: break-word;
        min-width: 120px;
        max-width: 200px;
        padding-left: 1em;
      }

      .results-table td[data-type="number"] {
        text-align: center;
        font-family: "Monaco", "Menlo", "Ubuntu Mono", monospace;
        font-weight: 500;
      }

      .results-table thead th {
        background: linear-gradient(135deg, #e2e8f0 0%, #cbd5e0 100%);
        color: #2d3748;
        font-weight: 600;
        font-size: 0.85rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        position: sticky;
        top: 0;
        z-index: 10;
      }

      .results-table tbody tr {
        transition: background-color 0.2s ease;
      }

      .results-table tbody tr:hover {
        background-color: #f7fafc;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
      }

      .results-table tbody tr:nth-child(even) {
        background-color: #f9fafb;
      }

      .best-result {
        font-weight: bold;
      }
      .code-block {
        background-color: #1e1e1e;
        color: #d4d4d4;
        padding: 1rem;
        border-radius: 4px;
        overflow-x: auto;
        font-family: "Monaco", "Menlo", "Ubuntu Mono", monospace;
        font-size: 0.875rem;
        margin: 1rem 0;
      }

      .architecture-diagram {
        max-width: 100%;
        margin: 1.5rem auto;
        display: block;
        background-color: #ffffff;
        border-radius: 8px;
        padding: 1rem;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      }

      .navbar {
        background-color: #fff;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        position: sticky;
        top: 0;
        z-index: 100;
      }

      .navbar-item {
        font-weight: 500;
      }

      .navbar-item:hover {
        background-color: #f5f5f5;
      }

      .text-content-constrained {
        max-width: 800px;
        margin: 0 auto;
      }

      .model-legend {
        display: flex;
        justify-content: center;
        gap: 1rem;
        flex-wrap: wrap;
        margin-top: 1rem;
        font-size: 0.9rem;
      }

      .model-legend-item {
        display: flex;
        align-items: center;
        gap: 0.4rem;
      }

      .model-legend-swatch {
        width: 14px;
        height: 14px;
        border-radius: 50%;
        border: 1px solid #d1d5db;
      }

      .model-legend-wrapper {
        max-width: 900px;
        margin: 0 auto 2rem;
      }
    </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
  </head>
  <body>
    <!-- Navigation -->
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a class="navbar-item" href="#home">
          <strong>Agent Interface Comparison</strong>
        </a>
      </div>

      <div class="navbar-menu">
        <div class="navbar-start">
          <a class="navbar-item" href="#motivation"> Motivation </a>
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> Agent Interfaces </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="#browser-based"> HTML Agent </a>
              <a class="navbar-item" href="#rag-alternative"> RAG Agent </a>
              <a class="navbar-item" href="#mcp"> MCP Agent </a>
              <a class="navbar-item" href="#nlweb-mcp"> NLWeb Agent </a>
              <a class="navbar-item" href="#technical"> Technical Details </a>
            </div>
          </div>
          <a class="navbar-item" href="#use-case"> Use Case </a>
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link" href="#results"> Results </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="#results-metrics"> Evaluation Metrics </a>
              <a class="navbar-item" href="#results-overall"> Overall Results </a>
              <a class="navbar-item" href="#results-interface-model"> Interface &amp; Model </a>
              <a class="navbar-item" href="#results-task-groups"> By Task Group </a>
              <a class="navbar-item" href="#results-cost-runtime"> Cost &amp; Runtime </a>
              <a class="navbar-item" href="#results-cost-effectiveness"> Cost vs Effectiveness </a>
            </div>
          </div>
          <a class="navbar-item" href="#running"> Running the Benchmark </a>
          <a class="navbar-item" href="#related-work"> Related Work </a>
        </div>
      </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero" id="home">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                MCP vs RAG vs NLWeb vs HTML:<br />
                A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/people/researchers/phd-students/aaron-steiner/">Aaron Steiner</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/people/researchers/postdoctoral-research-fellows/dr-ralph-peeters/">Ralph Peeters</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/people/professors/prof-dr-christian-bizer/">Christian Bizer</a>
                </span>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/research/focus-groups/web-based-systems-prof-bizer/">
                    Data and Web Science Group, University of Mannheim
                  </a>
                </span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://wbsg-uni-mannheim.github.io/WebMall/" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-chart-bar"></i>
                      </span>
                      <span>WebMall Benchmark</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Motivation Section -->
    <section class="section" id="motivation">
      <div class="container is-max-desktop">
        <!-- Introduction -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">1. Introduction</h2>
            <div class="content has-text-justified">
              <p>
                LLM-based agents are increasingly deployed to automate web tasks such as product search, offer comparison, and order placement. We see
                four dominant ways these agents interact with websites: (1) <b>HTML browsing</b> by clicking links and filling forms, (2)
                <b>retrieval-augmented generation (RAG)</b> over pre-crawled content, (3) <b>Model Context Protocol (MCP)</b> access to site-specific
                Web APIs, and (4) <b>NLWeb</b>, which lets agents issue natural-language queries that are answered with Schema.org-style JSON data.
                Despite rapid experimentation with these interfaces, the community still lacks a systematic comparison of their effectiveness and
                efficiency on the same challenging task sets.
              </p>
              <p>
                To close this gap we introduce a reproducible testbed with four simulated e-shops. Each shop offers its catalog via HTML, MCP, and
                NLWeb, while a dedicated crawler provides the corpus needed for the RAG interface. For every interface (HTML, RAG, MCP, NLWeb) we
                build a specialized agent so that all four agents face <b>exactly the same tasks</b> covering specific and vague product search,
                cheapest-offer comparisons, and transactional workflows such as cart management and checkout. The architecture diagram below provides
                an overview of the interfaces and their interaction patterns.
              </p>
              <p>
                Our study evaluates the agents with GPT-4.1, GPT-5, GPT-5-mini and Claude Sonnet 4, measuring task completion, precision/recall/F1,
                execution time, and token consumption to derive per-task cost estimates. Across search-oriented tasks, RAG, MCP, and NLWeb agents
                outperform the HTML baseline by roughly 11 percentage points in task completion while using 2–5× fewer tokens. The GPT-5 RAG agent
                delivers the best overall completion rate (0.79) with moderate token usage, while GPT-5-mini offers cost-effectiveness when absolute
                accuracy requirements are lower.
              </p>
              <p>This work makes two concrete contributions:</p>
              <ol>
                <li>
                  <b>Unified testbed</b>: We introduce a testbed for comparing different agent architectures. The testbed consists of four simulated
                  e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each of the four architectures (HTML, RAG, MCP, and
                  NLWeb) the testbed contains specialized agents that interact with the e-shops using the respective interfaces.
                </li>
                <li>
                  <b>Systematic evaluation</b>: Using different sets of challenging e-commerce tasks, we systematically evaluate agent performance
                  across interfaces and models (GPT-4.1, GPT-5, GPT-5-mini, Claude Sonnet 4) and analyze the effectiveness, efficiency, and cost of
                  the different agents.
                </li>
              </ol>
              <p>
                The remainder of this page summarizes the architectures, describes the task design, and presents the experimental findings. All code
                and data needed to reproduce the results are available in our
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces" target="_blank">GitHub repository</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Architecture Diagram -->
    <section class="section" id="architecture">
      <div class="container is-max-desktop">
        <h2 class="title is-3">2. Architectures and Interfaces</h2>
        <div class="content">
          <p>
            The section below describes the four different architectures that we compare in our experimental study as well as the interfaces that
            agents and Webshops use for communication.
          </p>
        </div>
        <img src="static/images/interface_overview.png" alt="" />
      </div>
    </section>

    <!-- Browser-based Section -->
    <section class="section" id="browser-based">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.1 Browser-based Agent (HTML Agent)</h3>
            <div class="content">
              <p>
                Within this architecture, e-shops expose traditional HTML interfaces intended for human consumption. The agents interact with these
                pages by clicking hyperlinks and performing form-filling actions. For our experiments we use the AX+MEM agent from the
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/" target="_blank">WebMall benchmark</a>, which is implemented with the
                <a href="https://github.com/ServiceNow/AgentLab" target="_blank">AgentLab</a> library and executed inside
                <a href="https://github.com/ServiceNow/BrowserGym" target="_blank">BrowserGym</a>. The agent observes the accessibility tree (AXTree)
                of each page and can store relevant information in short-term memory to maintain context across multiple steps. We disable visual
                perception in our setup because adding screenshots on top of the AXTree reduced performance in prior WebMall experiments.
              </p>

              <h4 class="title is-5">HTML Agent Workflow</h4>
              <p>The HTML agent executes the following interaction loop:</p>
              <ol>
                <li>Navigate to target web page using browser automation</li>
                <li>Parse accessibility tree (AXTree) to understand page content</li>
                <li>Store relevant information in short-term memory</li>
                <li>Execute action (click, type, scroll) based on task requirements</li>
                <li>Repeat interaction cycle until task completion</li>
              </ol>

              <p>
                More details about the AX+MEM HTML Agent are on the
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/" target="_blank">WebMall benchmark</a> page.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- RAG Section -->
    <section class="section" id="rag-alternative">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.2 Agent querying a Search Engine (RAG Agent)</h3>
            <div class="content">
              <p>
                The RAG architecture includes a search engine that crawls the HTML interfaces of all four shops, strips navigation and markup, and
                indexes the remaining text. Our implementation builds a unified Elasticsearch index fed by the
                <a href="https://github.com/Unstructured-IO/unstructured" target="_blank">Unstructured</a> processing pipeline. The RAG agent
                retrieves content by issuing queries to this search engine instead of visiting the live sites. It iteratively formulates queries,
                inspects the returned documents, and refines follow-up queries as needed. For transactional steps (add-to-cart or checkout) we expose
                dedicated Python functions that the agent can invoke directly.
              </p>
              <p>
                The agent leverages the <a href="https://www.langchain.com/langgraph" target="_blank">LangGraph</a> framework to orchestrate retrieval
                workflows and incorporates specialized tools for e-commerce actions. Concretely, the RAG agent is a LangGraph ReAct agent with the
                following capabilities:
              </p>
              <ol>
                <li><strong>search_products</strong>: Execute semantic search queries against Elasticsearch (returns title + URL for efficiency)</li>
                <li><strong>get_product_details</strong>: Fetch detailed product information for specific URLs</li>
                <li><strong>add_to_cart_webmall_1-4</strong>: Add products to specific shop carts</li>
                <li><strong>checkout_webmall_1-4</strong>: Complete purchases with customer details</li>
              </ol>
              <h4 class="title is-5">RAG Agent Workflow</h4>
              <p>
                The agent follows a multi-query process: it first issues lightweight searches to gather candidates, inspects the responses, and
                reformulates queries as many times as needed to cover all shops. Only after narrowing down promising items does it call
                <code>get_product_details</code> for detailed attributes, minimizing token usage.
              </p>

              <p>
                For more details about the implementation of the RAG agent please refer to the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src/rag">RAG source code</a>
                in our repository. The prompt used for defining the agent can be found
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/src/benchmark_rag.py#L373" target="_blank"> here</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- MCP Section -->
    <section class="section" id="mcp">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.3 Agent querying Web APIs via MCP (MCP Agent)</h3>
            <div class="content">
              <p>
                In the Model Context Protocol (MCP) architecture, the e-shops expose their product search, cart manipulation, and checkout
                functionality via proprietary APIs. Each shop hosts its own MCP server that defines the available functions and parameters, and the
                agent invokes these endpoints over the protocol instead of scraping HTML content.
              </p>
              <p>
                Like the RAG setup, the MCP agent can iteratively refine its queries: it issues a search, inspects the JSON response, and adjusts the
                next request accordingly. However, every shop uses its own function names, parameter conventions, and response schema. This
                heterogeneity places the burden of normalization on the agent, which must interpret different JSON formats when comparing or merging
                results across shops.
              </p>
              <p>
                Our MCP agent reuses the <a href="https://www.langchain.com/langgraph" target="_blank">LangGraph</a> framework from the RAG setup but
                swaps in MCP tool calls for product search, cart management, and checkout operations.
              </p>

              <h4 class="title is-5">MCP Agent Workflow</h4>
              <p>
                The MCP server for each shop exposes its capabilities as tools, which the agent can discover and execute. The workflow is as follows:
              </p>
              <ol>
                <li>
                  <strong>Connection and Discovery:</strong>
                  The agent (acting as an MCP Host) connects to the shop-specific MCP server and enumerates the available tools and parameters.
                </li>
                <li>
                  <strong>Iterative Tool Execution:</strong> The agent invokes search or cart functions with an initial query, inspects the JSON
                  result, and can reissue refined queries to explore additional items or shops.
                </li>
                <li>
                  <strong>Response Harmonization:</strong> Because each shop returns different attribute names and structures, the agent normalizes
                  the data before making comparisons or taking follow-up actions.
                </li>
              </ol>
              <p>
                The example below illustrates the heterogeneity of WebAPIs by comparing the search methods of two shops.
                <code>search_products</code> and <code>find_items_techtalk</code> use different parameter names, while the second method offers the
                possiblity to sort by price, which is not possible in the first method.
              </p>
              <div class="code-block">
                <pre>
// E-Store Athletes (Shop 1) search signature — no sorting
async def search_products(
    ctx: Context,
    query: str,
    per_page: int = 10,
    page: int = 1,
    include_descriptions: bool = False
) -> str

// TechTalk (Shop 2) search signature — supports sorting
async def find_items_techtalk(
    ctx: Context,
    query: str,
    limit: int = 5,
    page_num: int = 1,
    sort_by_price: str = "none",
    include_descriptions: bool = False
) -> str</pre
                >
              </div>
              <p>
                The heterogeneity extends beyond function signatures to the data structures that are returned by the different search endpoints, e.g.
                each shop used a different set of attribute names and its own product categorization hierarchy.
              </p>
              <p>
                For complete implementation details, refer to the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src/api_mcp">MCP server code</a>
                in our repository.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- NLWeb + MCP Section -->
    <section class="section" id="nlweb-mcp">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.4 Agent querying NLWeb Sites (NLWeb Agent)</h3>
            <div class="content">
              <p>
                The NLWeb interface extends MCP by requiring each shop to expose a standardized natural-language query endpoint. Every provider hosts
                an <code>ask</code> tool that accepts queries like “laptops under $1000 with 16GB RAM,” performs an internal search, and returns
                results in schema.org-style JSON. Because all shops respond with the same vocabulary, agents spend less effort harmonizing the data.
                Transactional workflows (cart actions, checkout) remain available via the shop’s MCP tools.
              </p>
              <p>
                In our deployment we create one Elasticsearch index per shop to support semantic search within the NLWeb server. Each query is
                embedded and matched against the shop’s own index before the results are serialized through the schema.org product vocabulary. For
                reference, a sample <code>ask</code> call and response is available
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/website/examples/nlweb_query.json" target="_blank">here</a
                >.
              </p>
              <p>
                The NLWeb agent uses the same <a href="https://www.langchain.com/langgraph" target="_blank">LangGraph</a> framework as our other
                agents, combining the standardized search tool with MCP-based cart and checkout functions. Table 2 later in this section summarizes
                the main characteristics of all four architectures.
              </p>
              <h4 class="title is-5">NLWeb Agent Workflow</h4>
              <p>The agent’s interaction with the NLWeb + MCP interface follows the workflow below:</p>
              <ol>
                <li>
                  <strong>Connection and Discovery:</strong>
                  The agent connects to the NLWeb-enabled MCP server for a specific shop and discovers the available tools.
                </li>
                <li>
                  <strong>Natural Language Query:</strong> The agent sends a natural language query (e.g., "laptops under $1000 with 16GB RAM") to the
                  server by invoking the <code>ask</code> tool.
                </li>
                <li>
                  <strong>Semantic Search Execution:</strong>
                  The server generates an embedding from the query and performs a cosine similarity search against the pre-computed vectors in its
                  dedicated Elasticsearch index.
                </li>
                <li><strong>Standardized Response:</strong> The agent receives a list of products in the standardized schema.org JSON format.</li>
              </ol>
              <p>
                The complete implementation is available in the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src/nlweb_mcp">NLWeb + MCP directory</a>. The prompt used
                for the agent can be found
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/src/benchmark_nlweb_mcp.py#L282" target="_blank">here</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Technical Details Section -->
    <section class="section" id="technical">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">2.5 Comparison of the Different Architectures</h2>
            <p>
              The table below compares the four different architectures along the features offered by the e-shops and the functionality implemented by
              the agents.
            </p>
            <div class="results-table">
              <table class="table is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Aspect</th>
                    <th>HTML</th>
                    <th>RAG</th>
                    <th>MCP</th>
                    <th>NLWeb</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <strong>E-Shops</strong>
                    </td>
                    <td colspan="4"></td>
                  </tr>
                  <tr>
                    <td>Interface</td>
                    <td>HTML pages</td>
                    <td>Retrieval API</td>
                    <td>Proprietary APIs</td>
                    <td>Standardized API</td>
                  </tr>
                  <tr>
                    <td>Search functionality</td>
                    <td>Free-text search per shop</td>
                    <td>Search engine over crawled content</td>
                    <td>Per-shop index; structured data</td>
                    <td>Per-shop index; structured data</td>
                  </tr>
                  <tr>
                    <td>Search response</td>
                    <td>HTML result list with links</td>
                    <td>Pre-processed HTML pages</td>
                    <td>Heterogeneous JSON</td>
                    <td>Schema.org JSON</td>
                  </tr>
                  <tr>
                    <td>
                      <strong>Agent</strong>
                    </td>
                    <td colspan="4"></td>
                  </tr>
                  <tr>
                    <td>Communication protocol</td>
                    <td>HTML over HTTP</td>
                    <td>Direct calls to search engine</td>
                    <td>JSON-RPC via MCP</td>
                    <td>JSON-RPC via MCP</td>
                  </tr>
                  <tr>
                    <td>Query strategy</td>
                    <td>Site search and browsing</td>
                    <td>Multi-query</td>
                    <td>Multi-query per shop</td>
                    <td>Multi-query per shop</td>
                  </tr>
                  <tr>
                    <td>Query refinement</td>
                    <td>Interactive page exploration</td>
                    <td>Self-evaluation &amp; iteration</td>
                    <td>Self-evaluation &amp; iteration</td>
                    <td>Self-evaluation &amp; iteration</td>
                  </tr>
                  <tr>
                    <td>Add to cart / checkout</td>
                    <td>Clicking &amp; form filling</td>
                    <td>Direct function calls</td>
                    <td>MCP tool invocation</td>
                    <td>MCP tool invocation</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Use Case: E-Commerce Section -->
    <section class="section" id="use-case">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">3. Use Case: Online Shopping</h2>
            <div class="content">
              <p>
                To evaluate the effectiveness and efficiency of the different agents, we use the
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/" target="_blank">WebMall benchmark</a>. WebMall simulates an online shopping
                environment with four distinct webshops, each offering around 1000 products described with heterogeneous product descriptions. The
                WebMall benchmark includes a diverse set of e-commerce tasks that test different agent capabilities ranging from focused retrieval to
                advanced reasoning about compatible and substitutional products. These tasks are organized into two main categories based on their
                complexity:
              </p>

              <h4 class="title is-5" style="margin-top: 2.5rem;">Specific Product Search (23 tasks)</h4>
              <p><em>Example: Find all offers for Fractal Design PC Gaming Cases which support 240mm radiators and 330mm GPUs.</em></p>
              <ul>
                <li><strong>Find Specific Product (12 tasks)</strong>: Locate a particular product by name or model number</li>
                <li>
                  <strong>Products Fulfilling Specific Requirements (11 tasks)</strong>: Find products matching precise technical specifications
                </li>
              </ul>

              <h4 class="title is-5" style="margin-top: 2.5rem;">Vague Product Search (19 tasks)</h4>
              <p><em>Example: Find all offers for compact keyboards that are best suited for working with a laptop remotely.</em></p>
              <ul>
                <li><strong>Products Satisfying Vague Requirements (8 tasks)</strong>: Interpret and fulfill imprecise or subjective requirements</li>
                <li><strong>Find Substitutes (6 tasks)</strong>: Identify alternative products when the requested item is unavailable</li>
                <li><strong>Find Compatible Products (5 tasks)</strong>: Locate accessories or components compatible with a given product</li>
              </ul>

              <h4 class="title is-5" style="margin-top: 2.5rem;">Cheapest Product Search (26 tasks)</h4>
              <p><em>Example: Find the cheapest offer for a new Xbox gaming console with at least 512 GB disk space in white.</em></p>
              <ul>
                <li><strong>Find Cheapest Offer (10 tasks)</strong>: Identify the lowest-priced option for a specific product across shops</li>
                <li>
                  <strong>Cheapest Offer with Specific Requirements (10 tasks)</strong>: Find the most affordable product meeting detailed criteria
                </li>
                <li><strong>Cheapest Offer with Vague Requirements (6 tasks)</strong>: Combine price optimization with fuzzy requirement matching</li>
              </ul>

              <h4 class="title is-5" style="margin-top: 2.5rem;">Transactional Tasks (15 tasks)</h4>
              <p><em>Example: Add the product on page {url} to the shopping cart and complete the checkout process.</em></p>
              <ul>
                <li><strong>Add to Cart (7 tasks)</strong>: Add selected products to the shopping cart</li>
                <li><strong>Checkout (8 tasks)</strong>: Complete the purchase process with payment and shipping information</li>
              </ul>

              <p>
                Further
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/#example-tasks" target="_blank">examples of each task type</a>
                are found on the WebMall benchmark page. The complete task set including the solution for each task is found in the
                <a
                  href="https://github.com/wbsg-uni-mannheim/BrowserGym/blob/main/browsergym/webmall/src/browsergym/webmall/task_sets.json"
                  target="_blank"
                  >WebMall repository</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Results Section -->
    <section class="section" id="results">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">4. Experimental Results</h2>
            <div class="content">
              <p>
                We evaluated each agent on the complete WebMall task set. The evaluation compares the effectiveness of different agent architectures:
                RAG Agent, MCP Agent, and NLWeb Agent. For comparison, we also include results from the strongest HTML Agent configuration from the
                original WebMall benchmark, <a href="https://wbsg-uni-mannheim.github.io/WebMall/#results" target="_blank">AX+MEM</a>.
              </p>
              <p>
                Every agent architecture is evaluated in combination with both GPT-4.1 (gpt-4.1-2025-04-14), GPT-5 (gpt-5-2025-08-07), GPT-5-mini
                (gpt-5-mini-2025-08-07) and Claude 4 Sonnet (claude-sonnet-4-20250514) to assess variations in effectiveness across models. Note that
                our experiments do not utilize prompt caching. Detailed execution logs for all runs are available in our
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/results" target="_blank">GitHub repository</a>, organized
                by interface type and model. For quick understanding of agent behavior,
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/results/demo-logs" target="_blank"
                  >shortened execution logs</a
                >
                containing one successful task execution per agent and model are also available.
              </p>

              <h3 class="title is-4" id="results-metrics">4.1 Evaluation Metrics</h3>
              <ul>
                <li>
                  <strong>Task completion rate (CR)</strong>: Computed as a binary success measure. For retrieval tasks, the set of URLs returned by the
                  agent must be identical to the test set. For transactional tasks (e.g., add-to-cart, checkout), completion requires reaching the
                  specified final state. This metric therefore captures strict task correctness.
                </li>
                <li>
                  <strong>Precision, Recall, F1</strong>: Computed by comparing the set of answers returned by the agent to the test set. Precision
                  measures the fraction of agent-returned items that are correct, recall measures the fraction of test set items recovered. These
                  metrics capture graded performance and are informative in cases of partial matches where completion rate alone would report failure.
                </li>
                <li>
                  <strong>Runtime (s)</strong>: End-to-end latency per task, measured from task submission to final output, including model reasoning
                  and all tool calls.
                </li>
                <li>
                  <strong>Token usage</strong>: Total number of input and output tokens consumed by the agent per task category. We explicitly exclude
                  embedding tokens used for indexing, as their cost is several orders of magnitude lower than LLM inference tokens and they provide
                  little information in this context.
                </li>
                <li>
                  <strong>Cost ($)</strong>: Estimated inference cost based on token usage, calculated using the published per-token input and output
                  prices from the respective model providers. The current agent implementations do not utilize prompt caching.
                </li>
              </ul>

              <h3 class="title is-4" id="results-overall">4.2 Overall Results</h3>
              <p>The following table shows the overall performance of each interface averaged across all tasks and models:</p>
              <div class="table-container">
                <div id="overall-results-table"></div>
              </div>
              <div class="text-content-constrained">
                <p>
                  <strong>Key insight:</strong> API-based (MCP: F1 0.75 and NLWeb: F1 0.76) and RAG (F1 0.76) outperform the HTML browsing agent (AX+MEM:
                  F1 0.69) by roughly 6-7 F1 points on average.
                </p>
              </div>

              <h3 class="title is-4" id="results-interface-model">4.3 Results by Interface and Model</h3>
              <p>
                The following table shows detailed results for each interface broken down by the model used. This allows for direct comparison of how
                different models perform with each interface architecture.
              </p>
              <div class="table-container">
                <div id="results-by-interface-and-model-table"></div>
              </div>
              <div class="text-content-constrained">
                <p><strong>Key insights</strong></p>
                <ul>
                  <li>
                    Across all interfaces, GPT-5 yields the highest combination of task completion and F1 (e.g., AX+MEM: CR 0.69/F1 0.76 vs. 0.55–0.59 CR and 0.64–0.68 F1 for other models; RAG: CR 0.79/F1 0.87 vs. 0.61–0.65 CR and 0.68–0.75 F1).
                  </li>
                  <li>
                    The GPT-5 RAG agent attains the best overall effectiveness in the table (CR 0.79, F1 0.87), outperforming GPT-5 MCP (CR 0.73, F1 0.82) and GPT-5 NLWeb (CR 0.73, F1 0.84).
                  </li>
                </ul>
              </div>

              <h3 class="title is-4" id="results-task-groups">4.4 Results by Task Group</h3>
              <h4 class="title is-5" style="margin-top: 2rem;">Specific Product Search</h4>
              <p>
                The Specific Product Search category includes tasks where users need to find particular products by name, model number, or specific
                technical requirements. This represents 23 tasks from the WebMall benchmark and tests the agents' ability to accurately retrieve and
                filter products based on precise criteria.
              </p>
              <div class="table-container">
                <div id="specific-product-search-table"></div>
              </div>
              <div class="text-content-constrained">
                <p><strong>Key insights</strong></p>
                <ul>
                  <li>
                    For every interface, GPT-5 delivers the strongest effectiveness: it yields the highest combination of completion rate and F1 compared to GPT-4.1, GPT-5-mini, and Claude Sonnet 4 within the same interface.
                  </li>
                  <li>
                    The best-performing configurations for specific product search are GPT-5 with RAG, MCP, and NLWeb (CR 0.83–0.87, F1 0.96), which all achieve very high precision and recall (P ≥ 0.95, R ≥ 0.95).
                  </li>
                  <li>
                    While different models show some variation within a given interface (e.g., RAG F1 ranges from 0.86 to 0.96, MCP from 0.84 to 0.96), the choice of interface has a larger impact on effectiveness in this task category: for the same model, RAG, MCP, and NLWeb consistently achieve higher CR and F1 than the HTML agent.
                  </li>
                </ul>
              </div>

              <h4 class="title is-5" style="margin-top: 3rem;">Vague Product Search</h4>
              <p>
                The Vague Product Search category includes 19 tasks that require interpreting imprecise or subjective requirements, finding product
                substitutes, and identifying compatible products. These tasks test the agents' ability to understand fuzzy requirements and apply
                semantic reasoning to match products that may not have exact specification matches.
              </p>
              <div class="table-container">
                <div id="vague-product-search-table"></div>
              </div>
              <div class="text-content-constrained">
                <p><strong>Key insights</strong></p>
                <ul>
                  <li>
                    Across interfaces and models, vague product search shows a clear drop in effectiveness compared to specific product search: the
                    average F1 decreases from about 0.87 to 0.76 (≈0.11 F1), and the average completion rate falls from about 0.73 to 0.60 (≈0.13 CR).
                  </li>
                  <li>
                    RAG with GPT-5 remains the strongest configuration (CR 0.79, F1 0.90), with NLWeb and MCP using GPT-5 or GPT-5-mini (F1 0.82–0.86), while the HTML agent with GPT-5 reaches F1 0.83.
                  </li>
                  <li>
                    For GPT-5, the performance gap between interfaces narrows compared to specific product search: AX+MEM, MCP, and NLWeb cluster
                    relatively closely on F1 (0.82–0.86), but RAG still leads with F1 0.90 on vague queries.
                  </li>
                  <li>
                    Within each interface, models vary in effectiveness (e.g., RAG F1 ranges from 0.68 to 0.90, NLWeb from 0.73 to 0.86), yet the
                    interface design continues to matter: for GPT-4.1 and Claude Sonnet 4, RAG, MCP, and NLWeb consistently reach higher F1 than the
                    HTML agent on vague product search.
                  </li>
                </ul>
              </div>

              <h4 class="title is-5" style="margin-top: 3rem;">Cheapest Product Search</h4>
              <p>
                The Cheapest Product Search category includes 26 tasks focused on finding the most affordable options. These tasks test the agents'
                ability to compare prices across products and shops, while also filtering by specific or vague requirements. This category combines
                price optimization with product search capabilities.
              </p>
              <div class="table-container">
                <div id="cheapest-product-search-table"></div>
              </div>
              <div class="text-content-constrained">
                <p><strong>Key insights</strong></p>
                <ul>
                  <li>
                    Cheapest product search is the most challenging category: averaged over all interfaces and models, F1 drops from about 0.88 on
                    specific product search and 0.77 on vague search to about 0.64 here (≈0.24 and ≈0.13 F1 lower, respectively), while average
                    completion rate decreases from roughly 0.74 to 0.60 (≈0.14 CR).
                  </li>
                  <li>
                    RAG with GPT-5 remains the top-performing configuration (CR 0.72, F1 0.78), with RAG+GPT-5-mini (CR 0.69, F1 0.76) and NLWeb+GPT-5
                    (CR 0.65, F1 0.75) close behind, indicating that several interfaces can reach similar effectiveness on price-focused tasks.
                  </li>
                  <li>
                    For GPT-5, the performance gap between interfaces narrows further compared to the other task groups: AX+MEM, RAG, MCP, and NLWeb
                    all achieve F1 scores between 0.72 and 0.78, so the best and worst GPT-5 setups differ by only 0.06 F1.
                  </li>
                  <li>
                    Interface design still matters, especially for weaker models: with GPT-4.1, RAG, MCP, and the HTML agent attain F1 scores between
                    0.58 and 0.68, whereas NLWeb+GPT-4.1 lags noticeably at 0.42 F1.
                  </li>
                </ul>
              </div>

              <h4 class="title is-5" style="margin-top: 3rem;">Actions and Transactions</h4>
              <p>
                The Actions and Transactions category includes 15 tasks focused on e-commerce operations: adding products to cart (7 tasks) and
                completing checkout processes (8 tasks). These tasks test the agents' ability to interact with transactional APIs and navigate
                multi-step workflows requiring sequential actions.
              </p>
              <div class="table-container">
                <div id="actions-transactions-table"></div>
              </div>
              <div class="text-content-constrained">
                <p><strong>Key insights</strong></p>
                <ul>
                  <li>
                    Transactional tasks are generally solved very reliably: averaged across all interfaces and models, completion rate is about 0.81
                    and F1 about 0.86, clearly higher than for any of the search-oriented task groups.
                  </li>
                  <li>
                    The HTML agent shows an unusual pattern: AX+MEM with GPT-4.1 achieves a perfect score (CR 1.00, F1 1.00), while GPT-5 and GPT-5-mini
                    drop to substantially lower effectiveness (CR 0.67/F1 0.64 and CR 0.53/F1 0.56, respectively).
                  </li>
                  <li>
                    For the other interfaces, the GPT-5 series performs strongly: RAG, MCP, and NLWeb with GPT-5 all reach CR 0.93 and F1 0.98, closely
                    matched by their GPT-4.1 and Claude Sonnet 4 counterparts (F1 0.96–0.98), indicating that executing structured action sequences is
                    a comparatively easy setting for these architectures.
                  </li>
                  <li>
                    GPT-5-mini consistently trails its larger counterparts on actions and transactions (e.g., RAG: F1 0.54, MCP: 0.88, NLWeb: 0.87),
                    suggesting that reduced model capacity has a more pronounced impact on multi-step transactional workflows than on many retrieval
                    tasks.
                  </li>
                </ul>
              </div>

              <h3 class="title is-4" id="results-cost-runtime">4.5 Cost & Runtime Analysis</h3>
              <p>Cost and execution time analysis based on model pricing and runtime results from our experiments.</p>
              <ul>
                <li>
                  <strong>GPT-4.1</strong>: $2.00/MTok input, $8.00/MTok output (<a
                    href="https://platform.openai.com/docs/models/gpt-4.1"
                    target="_blank"
                    >OpenAI pricing</a
                  >)
                </li>
                <li>
                  <strong>GPT-5</strong>: $2.00/MTok input, $8.00/MTok output (<a href="https://platform.openai.com/docs/models/gpt-5" target="_blank"
                    >OpenAI pricing</a
                  >)
                </li>
                <li>
                  <strong>GPT-5-mini</strong>: $0.20/MTok input, $0.60/MTok output (<a
                    href="https://platform.openai.com/docs/models/gpt-5-mini"
                    target="_blank"
                    >OpenAI pricing</a
                  >)
                </li>
                <li>
                  <strong>Claude Sonnet 4</strong>: $3.00/MTok input, $15.00/MTok output (<a
                    href="https://docs.anthropic.com/en/docs/about-claude/models/overview#model-comparison-table"
                    target="_blank"
                    >Anthropic pricing</a
                  >)
                </li>
              </ul>
              <p>
                The total cost of running all experiments across all interfaces and models was approximately
                <strong>$250</strong>. This cost excludes embedding generation (which is negligible at $0.02/MTok) and infrastructure costs. Execution
                times shown are averages per task.
              </p>

              <div id="cost-analysis">
                <!-- Cost tables will be inserted here -->
              </div>

              <h3 class="title is-4" id="results-cost-effectiveness">4.6 Cost vs Effectiveness Comparison</h3>
              <p>
                The scatter plot below visualizes the relationship between cost and effectiveness across different agent interfaces and models. Each
                point represents a combination of interface type (RAG, MCP, NLWeb, HTML) and language model (GPT-4.1, GPT-5, GPT-5-mini, Claude Sonnet 4).
              </p>
              <div class="text-content-constrained">
                <p><strong>Cost–effectiveness insights</strong></p>
                <ul>
                  <li>
                    RAG offers a particularly attractive price–performance trade-off. RAG with GPT-5-mini is by far the cheapest configuration in our
                    study (cost $0.01 with CR 0.65 and F1 0.75), while RAG with GPT-5 achieves the highest overall effectiveness (CR 0.79, F1 0.87) at a
                    still moderate cost of $0.15.
                  </li>
                  <li>
                    Among the API-based interfaces, NLWeb with GPT-5 combines strong effectiveness (CR 0.73, F1 0.84) with relatively low cost ($0.09),
                    whereas MCP with GPT-5 is slightly more expensive ($0.18, F1 0.82) but remains competitive in terms of performance.
                  </li>
                  <li>
                    The HTML agent (AX+MEM) is consistently less cost-efficient: even with GPT-5 it reaches lower effectiveness (F1 0.76) at a higher
                    cost ($0.50) than the best RAG and NLWeb configurations, while Claude Sonnet 4 with HTML is both the most expensive ($1.05) and not
                    the most effective option (F1 0.66).
                  </li>
                </ul>

                <p><strong>Runtime patterns</strong></p>
                <ul>
                  <li>
                    GPT-5 configurations tend to be noticeably slower than other models for the same interface: for example, AX+MEM with GPT-5 averages
                    522 s per task versus 92 s with GPT-4.1, and RAG with GPT-5 takes 114 s compared to 8 s with GPT-4.1.
                  </li>
                  <li>
                    Claude Sonnet 4 and GPT-4.1 generally offer the fastest execution times across interfaces (e.g., RAG+GPT-4.1: 8 s, MCP+GPT-4.1: 11 s,
                    NLWeb+Claude: 20 s), but GPT-4.1 in particular trades some effectiveness for speed (e.g., RAG F1 0.75 vs. 0.87 with GPT-5).
                  </li>
                  <li>
                    For many interfaces, GPT-5-mini sits between GPT-4.1 and GPT-5 in terms of runtime (e.g., RAG: 51 s vs. 8 s and 114 s; MCP: 80 s vs.
                    11 s and 94 s), offering moderate speed but also reduced effectiveness compared to full GPT-5.
                  </li>
                </ul>
              </div>
              <div style="max-width: 900px; margin: 2rem auto">
                <canvas id="costPerformanceChart" width="900" height="600"></canvas>
              </div>
              <div class="model-legend-wrapper">
                <div id="modelLegend" class="model-legend"></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Running the Benchmark Section -->
    <section class="section" id="running">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">5. Running the Benchmark</h2>
            <div class="content has-text-justified">
              <p>
                To reproduce our experiments or run the benchmarks with your own agent implementations, follow these setup instructions. The complete
                code and documentation are available in our
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces" target="_blank">GitHub repository</a>.
              </p>

              <h3 class="title is-4">Prerequisites</h3>
              <ul>
                <li><strong>Python 3.8+</strong>: Required for all agent implementations</li>
                <li><strong>Elasticsearch 8.x</strong>: Running on <code>http://localhost:9200</code></li>
                <li><strong>OpenAI API Key</strong>: For embeddings and LLM calls</li>
                <li><strong>Optional</strong>: Anthropic API key for Claude model support</li>
              </ul>

              <h3 class="title is-4">Quick Start</h3>
              <div class="code-block">
                <pre>
# Clone repository
git clone https://github.com/wbsg-uni-mannheim/WebMall-Interfaces.git
cd WebMall-Interfaces

# Install dependencies  
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Index data (required for NLWeb and API MCP)
cd src/nlweb_mcp
python ingest_data.py --shop all --force-recreate

# Run benchmarks
cd ..
python benchmark_nlweb_mcp.py    # NLWeb interface
python benchmark_rag.py          # RAG interface  
python benchmark_api_mcp.py      # API MCP interface</pre
                >
              </div>

              <p>
                For detailed setup instructions and interface-specific configuration, see the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/README.md" target="_blank">main README</a>
                and individual interface documentation in the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src" target="_blank">src/ directory</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Related Work Section -->
    <section class="section" id="related-work">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">6. Related Work</h2>
            <div class="content has-text-justified">
              <p>
                We discuss related work on architectures that LLM agents use to interact with websites. [<a href="#Song2025">Song 2025</a>] compares
                HTML browsing with direct API calling and shows that API agents achieve a 15% higher success rate on the WebArena benchmark [<a
                  href="#Zhou2023"
                  >Zhou 2023</a
                >] than HTML agents when reliable APIs exist. [<a href="#Lyu2025">Lyu 2025</a>] studies shopping tasks and contrasts browsing-based
                agents with RAG-based agents, reporting a ten-point F1 gain for RAG while noting declines for more complex comparison tasks. However,
                DeepShop evaluates on the live Web, which limits reproducibility. These papers corroborate that the interface strongly affects agent
                success but each compares at most two architectures. Our work extends this line of research by presenting the first controlled
                comparison of four architectures—HTML, RAG, MCP, and NLWeb—on identical tasks in a reproducible environment.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Feedback -->
    <section class="section" id="feedback">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">7. Feedback</h2>
            <div class="content has-text-justified">
              <p>
                We welcome feedback and contributions via GitHub issues and discussions. Alternatively, you can also contact us directly via email.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- References Section -->
    <section class="section" id="references">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">References</h2>
            <div class="content has-text-justified">
              <p id="Song2025">
                [Song 2025] Song, Yueqi, et al.:
                <a href="https://arxiv.org/abs/2410.16464" target="_blank"> Beyond Browsing: API-Based Web Agents</a>. arXiv:2410.16464, 2025.
              </p>
              <p id="Zhou2023">
                [Zhou 2023] Zhou, Shuyan, et al.:
                <a href="https://arxiv.org/abs/2307.13854" target="_blank"> WebArena: A Realistic Web Environment for Building Autonomous Agents</a>.
                arXiv:2307.13854, 2023.
              </p>
              <p id="Zhang2025">
                [Zhang 2025] Zhang, Chaoyun, et al.:
                <a href="https://arxiv.org/abs/2503.11069" target="_blank"> API Agents vs. GUI Agents: Divergence and Convergence</a>.
                arXiv:2503.11069, 2025.
              </p>
              <p id="Sager2025">
                [Sager 2025] Sager, Pascal, et al.:
                <a href="https://arxiv.org/abs/2501.16150" target="_blank"> A Comprehensive Survey of Agents for Computer Use</a>. arXiv:2501.16150,
                2025.
              </p>
              <p id="Yehudai2025">
                [Yehudai 2025] Yehudai, Asaf , et al.:
                <a href="https://arxiv.org/abs/2503.16416" target="_blank"> Survey on Evaluation of LLM-based Agents</a>, arXiv:2503.16416, 2025.
              </p>
              <p id="Petrova2025">
                [Petrova 2025] Petrova, Tatiana, et al.:
                <a href="https://arxiv.org/abs/2507.10644" target="_blank">
                  From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents</a
                >. arXiv:2507.10644, 2025.
              </p>
              <p id="Lyu2025">
                [Lyu 2025] Lyu, Yougang, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, Xiuying Chen:
                <a href="https://arxiv.org/abs/2506.02839" target="_blank"> DeepShop: A Benchmark for Deep Research Shopping Agents</a>.
                arXiv:2506.02839, 2025.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
          <p>
            This website is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
              >Creative Commons Attribution-ShareAlike 4.0 International License</a
            >.
          </p>
        </div>
      </div>
    </footer>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/interfaces.js"></script>
    <script src="./static/js/results-loader.js"></script>

    <script>
      // Cost vs Effectiveness Scatter Plot
      let costPerfChart = null;
      const MODEL_COLORS = {
        "GPT-4.1": "#36A2EB",
        "GPT-5": "#FF6384",
        "GPT-5-mini": "#9966FF",
        "Claude Sonnet 4": "#4BC0C0",
      };

      async function loadInterfaceResults() {
        try {
          const response = await fetch("./tables/results_by_interface_and_model.html");
          const html = await response.text();
          const parser = new DOMParser();
          const doc = parser.parseFromString(html, "text/html");
          const rows = Array.from(doc.querySelectorAll("tbody tr"));

          return rows
            .map((row) => {
              const cells = row.querySelectorAll("td");
              if (cells.length < 9) {
                return null;
              }

              const interfaceName = cells[0].textContent.trim();
              const model = cells[1].textContent.trim();
              const completionRate = parseFloat(cells[2].textContent.trim());
              const cost = parseFloat(cells[8].textContent.replace(/[^0-9.]/g, ""));

              if (isNaN(completionRate) || isNaN(cost)) {
                return null;
              }

              return {
                interfaceName,
                model,
                completionRate: completionRate * 100,
                cost,
              };
            })
            .filter(Boolean);
        } catch (error) {
          console.error("Failed to load interface results for chart:", error);
          return [];
        }
      }

      function buildCostEffectivenessDatasets(points) {
        const interfaceShapes = {
          "AX+MEM": "triangle",
          "RAG Agent": "circle",
          "API (MCP)": "rect",
          NLWeb: "rectRot",
        };
        const defaultShape = "circle";
        const defaultColor = "#888888";
        const interfaceLegendColor = "#6b7280";

        const grouped = points.reduce((acc, point) => {
          if (!acc[point.interfaceName]) {
            acc[point.interfaceName] = [];
          }

          acc[point.interfaceName].push({
            x: point.cost,
            y: point.completionRate,
            model: point.model,
            color: MODEL_COLORS[point.model] || defaultColor,
          });
          return acc;
        }, {});

        return Object.entries(grouped).map(([interfaceName, dataPoints]) => {
          const shape = interfaceShapes[interfaceName] || defaultShape;
          const colors = dataPoints.map((point) => point.color);

          return {
            label: interfaceName,
            data: dataPoints.map((point) => ({
              x: point.x,
              y: point.y,
              model: point.model,
            })),
            backgroundColor: interfaceLegendColor,
            borderColor: interfaceLegendColor,
            pointBackgroundColor: colors,
            pointBorderColor: colors,
            pointHoverBackgroundColor: colors,
            pointHoverBorderColor: colors,
            pointStyle: shape,
            pointRadius: 10,
            pointHoverRadius: 12,
          };
        });
      }

      function renderModelLegend() {
        const container = document.getElementById("modelLegend");
        if (!container) return;
        container.innerHTML = "";

        Object.entries(MODEL_COLORS).forEach(([model, color]) => {
          const item = document.createElement("div");
          item.className = "model-legend-item";

          const swatch = document.createElement("span");
          swatch.className = "model-legend-swatch";
          swatch.style.backgroundColor = color;

          const label = document.createElement("span");
          label.textContent = model;

          item.appendChild(swatch);
          item.appendChild(label);
          container.appendChild(item);
        });
      }

      async function renderCostEffectivenessChart() {
        const ctx = document.getElementById("costPerformanceChart");
        if (!ctx) return;

        const points = await loadInterfaceResults();
        if (!points.length) {
          console.warn("No datapoints available for cost-effectiveness chart.");
          return;
        }

        if (costPerfChart) {
          costPerfChart.destroy();
        }

        const datasets = buildCostEffectivenessDatasets(points);
        const completionValues = points.map((p) => p.completionRate);
        const costValues = points.map((p) => p.cost);

        const yMin = Math.max(0, Math.floor(Math.min(...completionValues) / 5) * 5 - 5);
        const yMax = Math.min(100, Math.ceil(Math.max(...completionValues) / 5) * 5 + 5);
        const xMin = Math.max(0.01, Math.min(...costValues) * 0.7);
        const xMax = Math.max(2, Math.max(...costValues) * 1.3);

        costPerfChart = new Chart(ctx, {
          type: "scatter",
          data: {
            datasets,
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              title: {
                display: true,
                text: "Cost vs Effectiveness: All Interfaces & Models",
                font: {
                  size: 18,
                },
              },
              legend: {
                position: "right",
                labels: {
                  font: {
                    size: 12,
                  },
                  usePointStyle: true,
                  padding: 15,
                  generateLabels: function (chart) {
                    const labels = Chart.defaults.plugins.legend.labels.generateLabels(chart);
                    return labels.map((label) => ({
                      ...label,
                      fillStyle: "#6b7280",
                      strokeStyle: "#6b7280",
                    }));
                  },
                },
              },
              tooltip: {
                callbacks: {
                  title: function (items) {
                    const item = items[0];
                    return `${item.dataset.label} · ${item.raw.model}`;
                  },
                  label: function (context) {
                    return [`Cost: $${context.parsed.x.toFixed(2)} per task`, `Completion Rate: ${context.parsed.y.toFixed(1)}%`];
                  },
                },
              },
            },
            scales: {
              x: {
                type: "logarithmic",
                position: "bottom",
                title: {
                  display: true,
                  text: "Average Cost per Task ($) - Log Scale",
                  font: {
                    size: 14,
                  },
                },
                min: xMin,
                max: xMax,
                ticks: {
                  callback: function (value) {
                    if ([0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2].includes(value)) {
                      return "$" + Number(value).toFixed(2);
                    }
                    return "";
                  },
                },
              },
              y: {
                title: {
                  display: true,
                  text: "Task Completion Rate (%)",
                  font: {
                    size: 14,
                  },
                },
                min: yMin,
                max: yMax,
                ticks: {
                  callback: function (value) {
                    return value + "%";
                  },
                  stepSize: 5,
                },
              },
            },
          },
        });

        renderModelLegend();
      }

      document.addEventListener("DOMContentLoaded", renderCostEffectivenessChart);
    </script>
  </body>
</html>
