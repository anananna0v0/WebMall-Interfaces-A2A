<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="description" content="Comparing Agent Interfaces for Web-based Agents on the WebMall Benchmark" />
    <meta name="keywords" content="WebMall, Agent Interfaces, RAG, MCP, NLWeb, LLM Agents, Benchmark Evaluation" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>MCP vs RAG vs NLWeb vs HTML: An Experimental Comparison of Different Agent Interfaces to the Web</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/favicon.ico" />

    <style>
      .results-table {
        position: relative;
        width: 90vw;
        max-width: 1400px;
        left: 50%;
        transform: translateX(-50%);
        overflow-x: auto;
        margin: 1.5rem 0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        border-radius: 8px;
      }

      .results-table table {
        width: 100%;
        table-layout: auto;
        font-size: 0.9rem;
        border-collapse: separate;
        border-spacing: 0;
        background: white;
        border-radius: 8px;
        overflow: hidden;
      }

      .results-table th,
      .results-table td {
        padding: 0.75em 0.5em;
        text-align: center;
        border-bottom: 1px solid #e5e7eb;
        vertical-align: middle;
        white-space: nowrap;
      }

      .results-table th:first-child,
      .results-table td:first-child {
        text-align: left;
        white-space: normal;
        word-wrap: break-word;
        min-width: 120px;
        max-width: 200px;
        padding-left: 1em;
      }

      .results-table td[data-type="number"] {
        text-align: center;
        font-family: "Monaco", "Menlo", "Ubuntu Mono", monospace;
        font-weight: 500;
      }

      .results-table thead th {
        background: linear-gradient(135deg, #e2e8f0 0%, #cbd5e0 100%);
        color: #2d3748;
        font-weight: 600;
        font-size: 0.85rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        position: sticky;
        top: 0;
        z-index: 10;
      }

      .results-table tbody tr {
        transition: background-color 0.2s ease;
      }

      .results-table tbody tr:hover {
        background-color: #f7fafc;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
      }

      .results-table tbody tr:nth-child(even) {
        background-color: #f9fafb;
      }

      .best-result {
        font-weight: bold;
      }
      .code-block {
        background-color: #1e1e1e;
        color: #d4d4d4;
        padding: 1rem;
        border-radius: 4px;
        overflow-x: auto;
        font-family: "Monaco", "Menlo", "Ubuntu Mono", monospace;
        font-size: 0.875rem;
        margin: 1rem 0;
      }

      .architecture-diagram {
        max-width: 100%;
        margin: 1.5rem auto;
        display: block;
        background-color: #ffffff;
        border-radius: 8px;
        padding: 1rem;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      }

      .navbar {
        background-color: #fff;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        position: sticky;
        top: 0;
        z-index: 100;
      }

      .navbar-item {
        font-weight: 500;
      }

      .navbar-item:hover {
        background-color: #f5f5f5;
      }

      .text-content-constrained {
        max-width: 800px;
        margin: 0 auto;
      }
    </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
  </head>
  <body>
    <!-- Navigation -->
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a class="navbar-item" href="#home">
          <strong>Agent Interface Comparison</strong>
        </a>
      </div>

      <div class="navbar-menu">
        <div class="navbar-start">
          <a class="navbar-item" href="#motivation"> Motivation </a>
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> Agent Interfaces </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="#rag-alternative"> RAG Agent </a>
              <a class="navbar-item" href="#mcp"> MCP Agent </a>
              <a class="navbar-item" href="#nlweb-mcp"> NLWeb Agent </a>
            </div>
          </div>
          <a class="navbar-item" href="#technical"> Technical Details </a>
          <a class="navbar-item" href="#use-case"> Use Case </a>
          <a class="navbar-item" href="#results"> Results </a>
          <a class="navbar-item" href="#running"> Running the Benchmark </a>
          <a class="navbar-item" href="#related-work"> Related Work </a>
          <a class="navbar-item" href="#references"> References </a>
          <a class="navbar-item" href="#feedback"> Feedback </a>
        </div>
      </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero" id="home">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                MCP vs RAG vs NLWeb vs HTML:<br />
                A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/people/researchers/phd-students/aaron-steiner/">Aaron Steiner</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/people/researchers/postdoctoral-research-fellows/dr-ralph-peeters/">Ralph Peeters</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/people/professors/prof-dr-christian-bizer/">Christian Bizer</a>
                </span>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://www.uni-mannheim.de/dws/research/focus-groups/web-based-systems-prof-bizer/">
                    Data and Web Science Group, University of Mannheim
                  </a>
                </span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://wbsg-uni-mannheim.github.io/WebMall/" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-chart-bar"></i>
                      </span>
                      <span>WebMall Benchmark</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Motivation Section -->
    <section class="section" id="motivation">
      <div class="container is-max-desktop">
        <!-- Introduction -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">1. Introduction</h2>
            <div class="content has-text-justified">
              <p>
                LLM agents use different architectures and interfaces to interact with the World Wide Web. Some agents rely on traditional web
                browsers to
                <b>navigate HTML pages</b> originally designed for human users. Others do not directly access websites but instead retrieve web
                content by <b>querying search engines</b> that have indexed the Web. A third architectural approach assumes that websites expose
                <b>site-specific Web APIs</b>, which agents interact with via the Model Context Protocol (MCP). A fourth architecture, proposed by
                Microsoft under the <b>NLWeb initiative</b>, defines a standardized interface through which agents query individual websites and
                receive responses formatted as structured Schema.org data.
              </p>
              <p>
                This page presents the results of the first experimental comparison of these four architectures, using the
                <b>same set of tasks</b> within an e-commerce scenario. The experiments were conducted across four simulated webshops, each offering
                products via different interfaces. Four corresponding LLM agents — the MCP Agent, RAG Agent, NLWeb Agent, and HTML Agent — are
                evaluated performing the same set of 91 tasks, each using a different method for interacting with the shops.
              </p>
              <p>
                We compare the <b>effectiveness</b> (success rate, F1) of the different agents in solving the tasks, which are grouped into categories
                such as searching for specific products, searching for the cheapest product given concrete or vague requirements, adding products to
                shopping carts, and finally checking out the products and paying for them by credit card. We also assess the <b>efficiency</b> of each
                architecture by measuring task runtime and token usage. The analysis of input and output tokens provides a basis for estimating both
                the operational cost of each agent as well as its energy consumption and environmental impact.
              </p>
              <p>
                In our WebMall experiments, the MCP, RAG, and NLWeb Agents achieve comparable — and in many cases higher — task completion rates than
                the HTML Agent (AX+MEM), while using 5–10× fewer tokens. On basic tasks, the NLWeb Agent achieves the highest completion rate (up to
                88% with Claude 4 Sonnet), and the RAG Agent shows competitive effectiveness across both basic and advanced tasks. All three
                alternatives use significantly fewer tokens than the HTML Agent.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Architecture Diagram -->
    <section class="section" id="architecture">
      <div class="container is-max-desktop">
        <h2 class="title is-3">2. Architectures and Interfaces</h2>
        <div class="content">
          <p>
            The section below describes the four different architectures that we compare in our experimental study as well as the interfaces that
            agents and Webshops use for communication.
          </p>
        </div>
      </div>
    </section>

    <!-- Browser-based Section -->
    <section class="section" id="browser-based">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.1 Browser-based Agent (HTML Agent)</h3>
            <div class="content">
              <p>
                The HTML Agent accesses the Webshops via their traditional HTML interfaces intended for human use. We employ the AX+MEM HTML Agent
                from the
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/" target="_blank">WebMall benchmark</a> for our experiments. The agent is
                implemented using the
                <a href="https://github.com/ServiceNow/AgentLab" target="_blank">AgentLab</a>
                library which accompanies
                <a href="https://github.com/ServiceNow/BrowserGym" target="_blank">BrowserGym</a>. The agent uses the accessibility tree (AXTree) of
                HTML pages as observation space and has access to short-term memory, which it can use to store relevant information at each step in
                order to maintain context across longer task sequences.
              </p>

              <h4 class="title is-5">HTML Agent Workflow</h4>
              <p>The HTML agent executes the following interaction loop:</p>
              <ol>
                <li>Navigate to target web page using browser automation</li>
                <li>Parse accessibility tree (AXTree) to understand page content</li>
                <li>Store relevant information in short-term memory</li>
                <li>Execute action (click, type, scroll) based on task requirements</li>
                <li>Repeat interaction cycle until task completion</li>
              </ol>

              <p>
                More details about the AX+MEM HTML Agent are on the
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/" target="_blank">WebMall benchmark</a> page.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- RAG Section -->
    <section class="section" id="rag-alternative">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.2 Agent querying a Search Engine (RAG Agent)</h3>
            <div class="content">
              <p>
                The RAG Agent does not directly access the Webshops but interacts with a search engine that has crawled and indexed all pages of all
                Webshops. Our RAG implementation uses
                <a href="https://www.elastic.co/elasticsearch/vector-database" target="_blank">Elasticsearch</a>
                to create a unified search index containing scraped content from all four WebMall shops. Before indexing, we remove navigation
                elements and HTML tags from the pages using the
                <a href="https://github.com/Unstructured-IO/unstructured" target="_blank">Unstructured library</a>. An example of a resulting JSON
                file can be found
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/website/examples/rag_elastic.json" target="_blank">here</a
                >. The system generates composite embeddings that combine product titles and descriptions, enabling semantic similarity search. The
                search engine is presented to the agent as a tool that can be called one or multiple times with differing queries to iteratively
                refine the results.
              </p>
              <p>
                The agent leverages the
                <a href="https://www.langchain.com/langgraph" target="_blank">LangGraph</a>
                framework to orchestrate retrieval workflows and incorporates specialized Python functions for e-commerce actions like adding items to
                carts and completing checkouts. More specifically, the RAG agent is implemented as a LangGraph ReAct agent with the following
                available tools:
              </p>
              <ol>
                <li><strong>search_products</strong>: Execute semantic search queries against Elasticsearch (returns title + URL for efficiency)</li>
                <li><strong>get_product_details</strong>: Fetch detailed product information for specific URLs</li>
                <li><strong>add_to_cart_webmall_1-4</strong>: Add products to specific shop carts</li>
                <li><strong>checkout_webmall_1-4</strong>: Complete purchases with customer details</li>
              </ol>
              <h4 class="title is-5">RAG Agent Workflow</h4>
              <p>
                The agent follows a two-phase search approach: first using lightweight searches to identify promising products, then fetching detailed
                information only for relevant items to minimize token usage.
              </p>

              <p>
                For more details about the implementation of the RAG agent please refer to the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src/rag">RAG source code</a>
                in our repository. The prompt used for defining the agent can be found
                <a
                  href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/src/rag/benchmark_v2_improved_langgraph.py#L370"
                  target="_blank"
                >
                  here</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- MCP Section -->
    <section class="section" id="mcp">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.3 Agent querying Web APIs via MCP (MCP Agent)</h3>
            <div class="content">
              <p>
                This architecture assumes that the e-shops provide proprietary API as well as MCP descriptions of the APIs. The agent interacts with the e-shops by calling shop-specific functions. These APIs are exposed via the
                <a href="https://modelcontextprotocol.io" target="_blank">Model Context Protocol (MCP)</a>, originally proposed by
                <a href="https://www.anthropic.com/news/model-context-protocol" target="_blank">Anthropic</a>. MCP is an open protocol designed to
                standardize communication between LLM applications and external tools or data sources. Instead of parsing unstructured web content, an
                agent (the MCP <em>Host</em>) connects to a dedicated MCP <em>Server</em> that exposes a well-defined set of tools (functions). 
              </p>
              <p>
                In our setup, we run four independent MCP servers, one for each WebMall shop. These servers expose tools for actions like search, cart
                management, and checkout. However, to simulate a realistic, multi-shop environment, the WebAPIs are heterogeneous - the data
                format and tools returned by each server are intentionally different. This heterogeneity forces the agent to adapt to different API
                responses from each shop, testing its ability to handle diverse, non-standardized data structures, which reflects the reality of
                integrating with multiple independent web APIs.
              </p>
              <p>
                The MCP agent is implemented using the same <a href="https://www.langchain.com/langgraph" target="_blank">LangGraph</a> framework as the RAG agent.
                It uses MCP tools exposed by each shop's server for product search, cart management, and checkout operations.
              </p>

              <h4 class="title is-5">MCP Agent Workflow</h4>
              <p>
                The MCP server for each shop exposes its capabilities as tools, which the agent can discover and execute. The workflow is as follows:
              </p>
              <ol>
                <li>
                  <strong>Connection and Discovery:</strong>
                  The agent, acting as an MCP Host, establishes a connection with the MCP Server for a specific shop and discovers the available tools
                  through the protocol's capability negotiation.
                </li>
                <li>
                  <strong>Tool Execution:</strong> The agent invokes tools like <code>search_products</code> or <code>add_to_cart</code> by sending
                  JSON-RPC messages to the server. The server executes the corresponding actions.
                </li>
                <li><strong>Response Handling:</strong> The agent receives a structured but potentially heterogeneous JSON response.</li>
              </ol>
              <p>
                The example below illustrates the heterogeneity of WebAPIs by comparing the search methods of two shops. <code>search_products</code> and <code>find_items_techtalk</code> use different parameter names, while the second method offers the possiblity to sort by price, which is not possible in the first method.
              </p>
              <div class="code-block">
                <pre>
// E-Store Athletes (Shop 1) search signature — no sorting
async def search_products(
    ctx: Context,
    query: str,
    per_page: int = 10,
    page: int = 1,
    include_descriptions: bool = False
) -> str

// TechTalk (Shop 2) search signature — supports sorting
async def find_items_techtalk(
    ctx: Context,
    query: str,
    limit: int = 5,
    page_num: int = 1,
    sort_by_price: str = "none",
    include_descriptions: bool = False
) -> str</pre
                >
              </div>
              <p>
                The heterogeneity extends beyond function signatures to the data structures that are returned by the different search endpoints, e.g.
                each shop used a different set of attribute names and its own product categorization hierarchy.
              </p>
              <p>
                For complete implementation details, refer to the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src/api_mcp">MCP server code</a>
                in our repository.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- NLWeb + MCP Section -->
    <section class="section" id="nlweb-mcp">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4">2.4 Agent querying NLWeb Sites (NLWeb Agent)</h3>
            <div class="content">
              <p>
                The NLWeb agent interacts with the e-shops through a standardized natural language interface that needs to be offered by all NLWeb sites. Each e-shop
                must implement "ask" endpoint that accepts natural language queries and returns structured responses using the
                schema.org format.
                <a href="https://github.com/nlweb-ai/NLWeb" target="_blank">NLWeb</a>
                (Natural Language for Web), proposed and supported by
                <a href="https://www.microsoft.com/en-us/research/blog/nlweb-a-blueprint-for-web-agent-interfaces/" target="_blank">Microsoft</a>,
                provides a standardized mechanism for this interaction. It leverages existing semi-structured formats, particularly
                schema.org and RSS, to create a semantic layer over a site's content.
              </p>
              <p>
                In our implementation, we create one dedicated Elasticsearch index per e-shop (NLWeb Site) that enables semantic search of that
                site's content. Each NLWeb server processes natural language queries by generating embeddings and performing cosine similarity search
                against its shop-specific index. Additionally, we create an MCP server per shop to enable other functionality like cart management and
                checkout operations, complementing the <code>ask</code> tool for product search.
              </p>
              <p>
                The NLWeb Agent uses the same <a href="https://www.langchain.com/langgraph" target="_blank">LangGraph</a> framework as the other
                agents. An example of an <code>ask</code> tool call and response can be found
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/website/examples/nlweb_query.json" target="_blank">here</a
                >.
              </p>
              <h4 class="title is-5">NLWeb Agent Workflow</h4>
              <p>The agent’s interaction with the NLWeb + MCP interface follows the workflow below:</p>
              <ol>
                <li>
                  <strong>Connection and Discovery:</strong>
                  The agent connects to the NLWeb-enabled MCP server for a specific shop and discovers the available tools.
                </li>
                <li>
                  <strong>Natural Language Query:</strong> The agent sends a natural language query (e.g., "laptops under $1000 with 16GB RAM") to the
                  server by invoking the <code>ask</code> tool.
                </li>
                <li>
                  <strong>Semantic Search Execution:</strong>
                  The server generates an embedding from the query and performs a cosine similarity search against the pre-computed vectors in its
                  dedicated Elasticsearch index.
                </li>
                <li><strong>Standardized Response:</strong> The agent receives a list of products in the standardized schema.org JSON format.</li>
              </ol>
              <p>
                The complete implementation is available in the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src/nlweb_mcp">NLWeb + MCP directory</a>. The prompt used
                for the agent can be found
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/src/benchmark_nlweb_mcp.py#L282" target="_blank">here</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Technical Details Section -->
    <section class="section" id="technical">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">2.5 Comparison of the Different Architectures</h2>
			              <p>
                The table below compares the four different architectures along the features offered by the e-shops and the functionality implemented by the agents.
              </p>
            <div class="results-table">
              <table class="table is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Aspect</th>
                    <th>RAG</th>
                    <th>MCP</th>
                    <th>NLWeb</th>
                    <th>HTML</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <strong>E-Shops</strong>
                    </td>
                    <td colspan="4"></td>
                  </tr>
                  <tr>
                    <td>Interface offered</td>
                    <td>HTML pages</td>
                    <td>proprietary APIs</td>
                    <td>standaardized API</td>
                    <td>HTML pages + search box</td>
                  </tr>
                  <tr>
                    <td>Search Functonality</td>
                    <td>Not used as shops are crawled</td>
                    <td>Search product data</td>
                    <td>Search product data</td>
                    <td>Search free text</td>
                  </tr>
                  <tr>
                    <td>Search Response</td>
                    <td>N/A</td>
                    <td>heterogenous JSON</td>
                    <td>schema.org products</td>
                    <td>HTML result list</td>
                  </tr>
                  <tr>
                    <td>
                      <strong>Agent</strong>
                    </td>
                    <td colspan="4"></td>
                  </tr>
                  <tr>
                    <td>Communication Protocol</td>
                    <td>direct calls to search engine</td>
                    <td>JSON-RPC via MCP</td>
                    <td>JSON-RPC via MCP</td>
                    <td>HTML over HTTP</td>
                  </tr>
                  <tr>
                    <td>Query Strategy</td>
                    <td>Multi-query generation</td>
                    <td>Multi-query possible per shop</td>
                    <td>Multi-query possible per shop</td>
                    <td>Site search and browsing</td>
                  </tr>
                  <tr>
                    <td>Search Refinement</td>
                    <td>Self-evaluation & iteration</td>
                    <td>Self-evaluation & iteration</td>
                    <td>Self-evaluation & iteration</td>
                    <td>Interactive page exploration</td>
                  </tr>
                  <tr>
                    <td>Cart Management</td>
                    <td>direct functions calls</td>
                    <td>MCP tool invocation</td>
                    <td>MCP tool invocation</td>
                    <td>Browser interactions</td>
                  </tr>
                  <tr>
                    <td>Checkout Process</td>
                    <td>direct functions calls</td>
                    <td>MCP tool invocation</td>
                    <td>MCP tool invocation</td>
                    <td>Form filling & submission</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Use Case: E-Commerce Section -->
    <section class="section" id="use-case">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">3. Use Case: Online Shopping</h2>
            <div class="content">
              <p>
                To evaluate the effectiveness and efficiency of the different agents, we use the
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/" target="_blank">WebMall benchmark</a>. WebMall simulates an online shopping
                environment with four distinct webshops, each offering around 1000 products described with heterogeneous product descriptions. The
                WebMall benchmark includes a diverse set of e-commerce tasks that test different agent capabilities ranging from focused retrieval to
                advanced reasoning about compatible and substitutional products. These tasks are organized into two main categories based on their
                complexity:
              </p>

              <h4 class="title is-5">Basic Tasks</h4>
              <ul>
                <li><strong>Find Specific Product (12 tasks)</strong>: Locate a particular product by name or model number</li>
                <li><strong>Find Cheapest Offer (10 tasks)</strong>: Identify the lowest-priced option for a specific product across shops</li>
                <li>
                  <strong>Products Fulfilling Specific Requirements (11 tasks)</strong>: Find products matching precise technical specifications
                </li>
                <li><strong>Add to Cart (7 tasks)</strong>: Add selected products to the shopping cart</li>
                <li><strong>Checkout (8 tasks)</strong>: Complete the purchase process with payment and shipping information</li>
              </ul>

              <h4 class="title is-5">Advanced Tasks</h4>
              <ul>
                <li>
                  <strong>Cheapest Offer with Specific Requirements (10 tasks)</strong>: Find the most affordable product meeting detailed criteria
                </li>
                <li><strong>Products Satisfying Vague Requirements (8 tasks)</strong>: Interpret and fulfill imprecise or subjective requirements</li>
                <li><strong>Cheapest Offer with Vague Requirements (6 tasks)</strong>: Combine price optimization with fuzzy requirement matching</li>
                <li><strong>Find Substitutes (6 tasks)</strong>: Identify alternative products when the requested item is unavailable</li>
                <li><strong>Find Compatible Products (5 tasks)</strong>: Locate accessories or components compatible with a given product</li>
                <li><strong>End To End (8 tasks)</strong>: Complete full shopping workflows from search to checkout</li>
              </ul>

              <p>
                Further
                <a href="https://wbsg-uni-mannheim.github.io/WebMall/#example-tasks" target="_blank">examples of each task type</a>
                are found on the WebMall benchmark page. The complete task set including the solution for each task is found in the
                <a
                  href="https://github.com/wbsg-uni-mannheim/BrowserGym/blob/main/browsergym/webmall/src/browsergym/webmall/task_sets.json"
                  target="_blank"
                  >WebMall repository</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Results Section -->
    <section class="section" id="results">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">4. Experimental Results</h2>
            <div class="content">
              <p>
                We evaluated each agent on the complete WebMall task set. The evaluation compares the effectiveness of different
                agent architectures: RAG Agent, MCP Agent, and NLWeb Agent. For comparison, we also include results from the strongest HTML Agent
                configuration from the original WebMall benchmark, <a href="https://wbsg-uni-mannheim.github.io/WebMall/#results" target="_blank">AX+MEM</a>.
              </p>
              <p>
                Every agent architecture is evaluated in combination with both GPT-4.1 (gpt-4.1-2025-04-14) and Claude 4 Sonnet (claude-sonnet-4-20250514) to assess
                variations in effectiveness across models. Note that our experiments do not utilize prompt caching. Detailed execution logs for all
                runs are available in our
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/results/v1" target="_blank">GitHub repository</a>,
                organized by interface type and model. For quick understanding of agent behavior,
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/results/v1/shortened_logs" target="_blank"
                  >shortened execution logs</a
                >
                containing one successful task execution per agent and model are also available.
              </p>

              <h3 class="title is-4">4.1 Evaluation Metrics</h3>
              <p>We assess effectiveness using four metrics derived from comparing the agent's response against the ground truth solutions:</p>
              <ul>
                <li>
                  <strong>Task Completion Rate</strong>: Binary metric (0 or 1) measuring exact task completion. An agent achieves 1.0 only if no
                  correct product is missing and no additional products are returned.
                </li>
                <li>
                  <strong>Precision</strong>: Fraction of the products returned by the agent that are correct (intersection ÷ total products returned). Higher precision
                  means fewer incorrect products.
                </li>
                <li>
                  <strong>Recall</strong>: Fraction of correct products that the agent found (intersection ÷ total correct products). Higher recall means
                  fewer missing products.
                </li>
                <li><strong>F1 Score</strong>: Harmonic mean of precision and recall.</li>
              </ul>

              <h3 class="title is-4">4.2 Effectiveness by Task Type</h3>
              <p>
                The results are shown in the following tables, sorted by completion rate and categorized by task type. Best results per metric are
                highlighted in bold.
              </p>
              <div id="results-by-type" class="results-table">
                <!-- Results table will be inserted here -->
              </div>

              <div id="findings-4-2-basic" class="content">
                <h5 class="title is-5">Key Findings</h5>
                <ul>
                  <li>
                    NLWeb with Claude Sonnet achieves the highest completion rate (88%) for basic tasks, while using fewer tokens than the HTML Agent.
                  </li>
                  <li>RAG delivers strong basic-task effectiveness with substantially lower token usage than the HTML Agent.</li>
                  <li>The HTML agents consume 5–10× more tokens than other agents without consistent advantages in completion rate.</li>
                </ul>
              </div>

              <div id="findings-4-2-advanced" class="content">
                <h5 class="title is-5">Key Findings</h5>
                <ul>
                  <li>
                    HTML with Claude Sonnet attains the highest completion rate for advanced tasks (~49%) but with substantially higher token usage.
                  </li>
                  <li>Differences across models are pronounced in advanced tasks; Claude tends to outperform GPT 4.1 across all agent architectures.</li>
                </ul>
              </div>

              <h3 class="title is-4">4.3 Effectiveness by Category</h3>
              <p>
                Results grouped by task complexity:
                <strong>Basic</strong> tasks include straightforward operations like finding specific products and simple checkout processes, while
                <strong>Advanced</strong> tasks require complex reasoning such as interpreting vague requirements, finding substitutes, and compatible products.
              </p>
              <div id="results-by-category" class="results-table">
                <!-- Results table will be inserted here -->
              </div>

              <div id="findings-4-3-basic" class="content">
                <h5 class="title is-5">Key Findings</h5>
                <ul>
                  <li>
                    Find Specific Product: Structured/indexed approaches (RAG, NLWeb, MCP) reach or approach very high completion, while the HTML
                    agent trails on this category.
                  </li>
                  <li>Find Cheapest Offer: RAG achieves strong effectiveness with lower token usage than the HTML Agent.</li>
                  <li>Best Fit Specific: HTML underperforms compared to structured/indexed interfaces.</li>
                  <li>
                    Checkout: API- and tool-based interfaces (MCP, NLWeb, RAG) are more reliable and efficient than HTML for transactional steps.
                  </li>
                </ul>
              </div>

              <div id="findings-4-3-advanced" class="content">
                <h5 class="title is-5">Key Findings</h5>
                <ul>
                  <li>Best Fit Vague: NLWeb performs well on vague requirement matching when standardized data is available.</li>
                  <li>
                    Cheapest Best Fit Vague and Find Compatible Products: Completion rates are low across all agents, reflecting the difficulty of
                    these tasks.
                  </li>
                  <li>
                    End-to-End: Standardized product data and transactional APIs (NLWeb/MCP) enable strong end-to-end performance; RAG remains
                    efficient on extended tasks.
                  </li>
                </ul>
              </div>

              <h3 class="title is-4">4.4 Cost & Runtime Analysis</h3>
              <p>Cost and execution time analysis based on model pricing and runtime results from our experiments. Token prices as of July 2025:</p>
              <ul>
                <li>
                  <strong>GPT-4.1</strong>: $2.00/MTok input, $8.00/MTok output (<a
                    href="https://platform.openai.com/docs/models/gpt-4.1"
                    target="_blank"
                    >OpenAI pricing</a
                  >)
                </li>
                <li>
                  <strong>Claude 4 Sonnet</strong>: $3.00/MTok input, $15.00/MTok output (<a
                    href="https://docs.anthropic.com/en/docs/about-claude/models/overview#model-comparison-table"
                    target="_blank"
                    >Anthropic pricing</a
                  >)
                </li>
              </ul>
              <p>
                The total cost of running all experiments across all interfaces and models was approximately
                <strong>$250</strong>. This cost excludes embedding generation (which is negligible at $0.02/MTok) and infrastructure costs. Execution
                times shown are averages per task.
              </p>

              <div id="cost-analysis">
                <!-- Cost tables will be inserted here -->
              </div>

              <div id="findings-4-4-basic" class="content">
                <h5 class="title is-5">Key Findings</h5>
                <ul>
                  <li>RAG with GPT-4.1 shows a favorable cost–effectiveness profile on basic tasks.</li>
                  <li>NLWeb with Claude Sonnet provides the highest accuracy at higher cost and latency.</li>
                  <li>MCP with Claude Sonnet tends to be costlier due to higher prompt token usage.</li>
                </ul>
              </div>

              <div id="findings-4-4-advanced" class="content">
                <h5 class="title is-5">Key Findings</h5>
                <ul>
                  <li>HTML with Claude Sonnet achieves the highest completion rate, but at higher cost and runtime.</li>
                  <li>RAG with GPT-4.1 offers competitive completion with much lower cost and latency.</li>
                  <li>NLWeb with Claude Sonnet balances effectiveness and efficiency relative to MCP and HTML.</li>
                </ul>
              </div>

              <h3 class="title is-4">4.5 Cost vs Effectiveness Comparison</h3>
              <p>
                The scatter plot below visualizes the relationship between cost and effectiveness across different agent interfaces and models. Each
                point represents a combination of interface type (RAG, MCP, NLWeb, HTML) and language model (GPT-4.1, Claude 4 Sonnet).
              </p>
              <div style="text-align: center; margin: 1rem 0">
                <div class="buttons has-addons is-centered">
                  <button class="button is-primary is-selected" id="btnAll" onclick="updateScatterChart('all', this)">All Tasks</button>
                  <button class="button" id="btnBasic" onclick="updateScatterChart('basic', this)">Basic Tasks</button>
                  <button class="button" id="btnAdvanced" onclick="updateScatterChart('advanced', this)">Advanced Tasks</button>
                </div>
              </div>
              <div style="max-width: 900px; margin: 2rem auto">
                <canvas id="costPerformanceChart" width="900" height="600"></canvas>
              </div>

              <h3 class="title is-4">Key Findings</h3>
              <ul>
                <li>
                  <strong>Effectiveness:</strong> The task completion rates are mixed and strongly depend on the specific model/architecture combination. The RAG, MCP, and NLWeb agents often match or exceed the effectiveness of the HTML agent. 
				  The completion rates for the basic task set are rather high, with the NLWeb agent achieving the best result (88%).
				  For the advanced tasks, the completion rates remain below 50% for all model/architecture combinations. This shows that the agents can in principle also solve advanced tasks, but are still not relyable enough for practical deployment.
                </li>
				<li>
                  <strong>Efficiency:</strong> The RAG, MCP, and NLWeb agents use 5-10 times less tokens than the HTML agent, translating to significantly lower costs.
                </li>
                <li>
                  <strong>Runtime:</strong> RAG and MCP-based agents complete tasks faster due to direct data access without page navigation overhead.
                </li>
                <li>
                  <strong>Viability:</strong> GPT-4.1 with RAG delivers low average cost per task (about $0.02 on basic tasks in our runs) with solid
                  effectiveness (about 83% completion), making it a practical option for basic shopping tasks. For advanced tasks, effectiveness is
                  mixed across interfaces and models, so selection should consider task complexity and cost trade-offs.
                </li>

              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Running the Benchmark Section -->
    <section class="section" id="running">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">5. Running the Benchmark</h2>
            <div class="content has-text-justified">
              <p>
                To reproduce our experiments or run the benchmarks with your own agent implementations, follow these setup instructions. The complete
                code and documentation are available in our
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces" target="_blank">GitHub repository</a>.
              </p>

              <h3 class="title is-4">Prerequisites</h3>
              <ul>
                <li><strong>Python 3.8+</strong>: Required for all agent implementations</li>
                <li><strong>Elasticsearch 8.x</strong>: Running on <code>http://localhost:9200</code></li>
                <li><strong>OpenAI API Key</strong>: For embeddings and LLM calls</li>
                <li><strong>Optional</strong>: Anthropic API key for Claude model support</li>
              </ul>

              <h3 class="title is-4">Quick Start</h3>
              <div class="code-block">
                <pre>
# Clone repository
git clone https://github.com/wbsg-uni-mannheim/WebMall-Interfaces.git
cd WebMall-Interfaces

# Install dependencies  
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Index data (required for NLWeb and API MCP)
cd src/nlweb_mcp
python ingest_data.py --shop all --force-recreate

# Run benchmarks
cd ..
python benchmark_nlweb_mcp.py    # NLWeb interface
python benchmark_rag.py          # RAG interface  
python benchmark_api_mcp.py      # API MCP interface</pre
                >
              </div>

              <p>
                For detailed setup instructions and interface-specific configuration, see the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/blob/main/README.md" target="_blank">main README</a>
                and individual interface documentation in the
                <a href="https://github.com/wbsg-uni-mannheim/WebMall-Interfaces/tree/main/src" target="_blank">src/ directory</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Related Work Section -->
    <section class="section" id="related-work">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">6. Related Work</h2>
            <div class="content has-text-justified">
              <p>
                The closest related work to our experiments is [<a href="#Song2025">Song 2025</a>], which uses tasks from the WebArena benchmark to
                compare LLM agents that access websites via APIs with agents that browse HTML interfaces. The study concludes that API-based agents
                are more effective than HTML agents, with hybrid agents achieving the best overall performance. A second relevant study is [<a
                  href="#Zhang2025"
                  >Zhang 2025</a
                >], which compares API agents to GUI agents. This paper introduces a set of dimensions for comparing API and GUI agents and reports
                the results of an experimental evaluation using 27 office-related tasks, involving Word, Excel, and PPT. Complementary to these, [<a
                  href="#Lyu2025"
                  >Lyu 2025</a
                >] introduces <em>DeepShop</em>, a benchmark for shopping agents with complex, multi-attribute queries that require filters and
                sorting; it finds that RAG struggles without web interaction and that agents face difficulties with filters/sorting, resulting in low
                overall success rates. DeepShop evaluates on live e-commerce sites that change over time, so exact runs are less reproducible than the
                controlled, repeatable experiments in this work.
              </p>
              <p>
                A survey of agents for computer use, including Web agents, is presented by [<a href="#Sager2025">Sager 2025</a>]. [<a
                  href="#Yehudai2025"
                  >Yehudai 2025</a
                >] surveys benchmarks used for evaluating LLM agents, while [<a href="#Petrova2025">Petrova 2025</a>] discusses LLM-based agents in
                the context of the historical development of the Semantic Web and FIPA-based multi-agent systems.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Feedback -->
    <section class="section" id="feedback">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">7. Feedback</h2>
            <div class="content has-text-justified">
              <p>
                We welcome feedback and contributions via GitHub issues and discussions. Alternatively, you can also contact us directly via email.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- References Section -->
    <section class="section" id="references">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">References</h2>
            <div class="content has-text-justified">
              <p id="Song2025">
                [Song 2025] Song, Yueqi, et al.:
                <a href="https://arxiv.org/abs/2410.16464" target="_blank"> Beyond Browsing: API-Based Web Agents</a>. arXiv:2410.16464, 2025.
              </p>
              <p id="Zhang2025">
                [Zhang 2025] Zhang, Chaoyun, et al.:
                <a href="https://arxiv.org/abs/2503.11069" target="_blank"> API Agents vs. GUI Agents: Divergence and Convergence</a>.
                arXiv:2503.11069, 2025.
              </p>
              <p id="Sager2025">
                [Sager 2025] Sager, Pascal, et al.:
                <a href="https://arxiv.org/abs/2501.16150" target="_blank"> A Comprehensive Survey of Agents for Computer Use</a>. arXiv:2501.16150,
                2025.
              </p>
              <p id="Yehudai2025">
                [Yehudai 2025] Yehudai, Asaf , et al.:
                <a href="https://arxiv.org/abs/2503.16416" target="_blank"> Survey on Evaluation of LLM-based Agents</a>, arXiv:2503.16416, 2025.
              </p>
              <p id="Petrova2025">
                [Petrova 2025] Petrova, Tatiana, et al.:
                <a href="https://arxiv.org/abs/2507.10644" target="_blank">
                  From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents</a
                >. arXiv:2507.10644, 2025.
              </p>
              <p id="Lyu2025">
                [Lyu 2025] Lyu, Yougang, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, Xiuying Chen:
                <a href="https://arxiv.org/abs/2506.02839" target="_blank"> DeepShop: A Benchmark for Deep Research Shopping Agents</a>.
                arXiv:2506.02839, 2025.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
          <p>
            This website is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
              >Creative Commons Attribution-ShareAlike 4.0 International License</a
            >.
          </p>
        </div>
      </div>
    </footer>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/interfaces.js"></script>
    <script src="./static/js/results-loader.js"></script>

    <script>
      // Cost vs Effectiveness Scatter Plot
      let costPerfChart = null;

      // Effectiveness data for different task categories - from actual CSV results
      const performanceData = {
        all: {
          // Calculate all as weighted average: (basic * 48 + advanced * 43) / 91
          "RAG_GPT-4.1": { cost: 0.03, performance: 64 },
          "MCP_GPT-4.1": { cost: 0.07, performance: 49 },
          "NLWeb_GPT-4.1": { cost: 0.075, performance: 52 },
          "HTML_GPT-4.1": { cost: 0.345, performance: 56 },
          "RAG_Claude 4 Sonnet": { cost: 0.2, performance: 67 },
          "MCP_Claude 4 Sonnet": { cost: 0.75, performance: 63 },
          "NLWeb_Claude 4 Sonnet": { cost: 0.16, performance: 68 },
          "HTML_Claude 4 Sonnet": { cost: 1.155, performance: 61 },
        },
        basic: {
          "RAG_GPT-4.1": { cost: 0.02, performance: 83 },
          "MCP_GPT-4.1": { cost: 0.06, performance: 71 },
          "NLWeb_GPT-4.1": { cost: 0.06, performance: 69 },
          "HTML_GPT-4.1": { cost: 0.29, performance: 75 },
          "RAG_Claude 4 Sonnet": { cost: 0.11, performance: 85 },
          "MCP_Claude 4 Sonnet": { cost: 0.65, performance: 79 },
          "NLWeb_Claude 4 Sonnet": { cost: 0.13, performance: 88 },
          "HTML_Claude 4 Sonnet": { cost: 0.94, performance: 71 },
        },
        advanced: {
          "RAG_GPT-4.1": { cost: 0.04, performance: 44 },
          "MCP_GPT-4.1": { cost: 0.08, performance: 26 },
          "NLWeb_GPT-4.1": { cost: 0.09, performance: 33 },
          "HTML_GPT-4.1": { cost: 0.4, performance: 35 },
          "RAG_Claude 4 Sonnet": { cost: 0.29, performance: 47 },
          "MCP_Claude 4 Sonnet": { cost: 0.85, performance: 44 },
          "NLWeb_Claude 4 Sonnet": { cost: 0.19, performance: 46 },
          "HTML_Claude 4 Sonnet": { cost: 1.37, performance: 49 },
        },
      };

      function updateScatterChart(category, buttonElement) {
        // Update button states
        document.querySelectorAll(".buttons button").forEach((btn) => {
          btn.classList.remove("is-primary", "is-selected");
        });

        // Handle button element
        if (buttonElement) {
          buttonElement.classList.add("is-primary", "is-selected");
        } else {
          // Initial load - select the All Tasks button
          const allTasksButton = document.getElementById("btnAll");
          if (allTasksButton) {
            allTasksButton.classList.add("is-primary", "is-selected");
          }
        }

        const data = performanceData[category];
        const ctx = document.getElementById("costPerformanceChart");

        if (costPerfChart) {
          costPerfChart.destroy();
        }

        // Define different point styles for models
        const pointStyles = {
          "GPT-4.1": "circle",
          "Claude 4 Sonnet": "rect",
        };

        costPerfChart = new Chart(ctx, {
          type: "scatter",
          data: {
            datasets: [
              {
                label: "RAG (GPT-4.1)",
                data: [
                  {
                    x: data["RAG_GPT-4.1"].cost,
                    y: data["RAG_GPT-4.1"].performance,
                  },
                ],
                backgroundColor: "#FF6384",
                borderColor: "#FF6384",
                pointStyle: "circle",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
              {
                label: "RAG (Claude 4)",
                data: [
                  {
                    x: data["RAG_Claude 4 Sonnet"].cost,
                    y: data["RAG_Claude 4 Sonnet"].performance,
                  },
                ],
                backgroundColor: "#FF6384",
                borderColor: "#FF6384",
                pointStyle: "rect",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
              {
                label: "MCP (GPT-4.1)",
                data: [
                  {
                    x: data["MCP_GPT-4.1"].cost,
                    y: data["MCP_GPT-4.1"].performance,
                  },
                ],
                backgroundColor: "#36A2EB",
                borderColor: "#36A2EB",
                pointStyle: "circle",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
              {
                label: "MCP (Claude 4)",
                data: [
                  {
                    x: data["MCP_Claude 4 Sonnet"].cost,
                    y: data["MCP_Claude 4 Sonnet"].performance,
                  },
                ],
                backgroundColor: "#36A2EB",
                borderColor: "#36A2EB",
                pointStyle: "rect",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
              {
                label: "NLWeb (GPT-4.1)",
                data: [
                  {
                    x: data["NLWeb_GPT-4.1"].cost,
                    y: data["NLWeb_GPT-4.1"].performance,
                  },
                ],
                backgroundColor: "#4BC0C0",
                borderColor: "#4BC0C0",
                pointStyle: "circle",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
              {
                label: "NLWeb (Claude 4)",
                data: [
                  {
                    x: data["NLWeb_Claude 4 Sonnet"].cost,
                    y: data["NLWeb_Claude 4 Sonnet"].performance,
                  },
                ],
                backgroundColor: "#4BC0C0",
                borderColor: "#4BC0C0",
                pointStyle: "rect",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
              {
                label: "HTML (GPT-4.1)",
                data: [
                  {
                    x: data["HTML_GPT-4.1"].cost,
                    y: data["HTML_GPT-4.1"].performance,
                  },
                ],
                backgroundColor: "#FFCE56",
                borderColor: "#FFCE56",
                pointStyle: "circle",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
              {
                label: "HTML (Claude 4)",
                data: [
                  {
                    x: data["HTML_Claude 4 Sonnet"].cost,
                    y: data["HTML_Claude 4 Sonnet"].performance,
                  },
                ],
                backgroundColor: "#FFCE56",
                borderColor: "#FFCE56",
                pointStyle: "rect",
                pointRadius: 10,
                pointHoverRadius: 12,
              },
            ],
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              title: {
                display: true,
                text: `Cost vs Effectiveness: ${category.charAt(0).toUpperCase() + category.slice(1)} Tasks`,
                font: {
                  size: 18,
                },
              },
              legend: {
                position: "right",
                labels: {
                  font: {
                    size: 12,
                  },
                  usePointStyle: true,
                  padding: 15,
                },
              },
              tooltip: {
                callbacks: {
                  label: function (context) {
                    return [`Cost: $${context.parsed.x.toFixed(4)} per task`, `Completion Rate: ${context.parsed.y.toFixed(1)}%`];
                  },
                },
              },
            },
            scales: {
              x: {
                type: "logarithmic",
                position: "bottom",
                title: {
                  display: true,
                  text: "Average Cost per Task ($) - Log Scale",
                  font: {
                    size: 14,
                  },
                },
                min: 0.01,
                max: 2,
                ticks: {
                  callback: function (value) {
                    if ([0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2].includes(value)) {
                      return "$" + value.toFixed(2);
                    }
                    return "";
                  },
                },
              },
              y: {
                title: {
                  display: true,
                  text: "Task Completion Rate (%)",
                  font: {
                    size: 14,
                  },
                },
                min: category === "basic" ? 65 : category === "advanced" ? 20 : 45,
                max: category === "basic" ? 90 : category === "advanced" ? 55 : 70,
                ticks: {
                  callback: function (value) {
                    return value + "%";
                  },
                  stepSize: 5,
                },
              },
            },
          },
        });
      }

      document.addEventListener("DOMContentLoaded", function () {
        updateScatterChart("all");
      });
    </script>
  </body>
</html>
